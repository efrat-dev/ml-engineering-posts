---
language: "he"
title: "CUDA - הכלי שהפך את ה-GPU לנגיש לכולם"
categories:
  - "תוכנה"
tags:
  - "CUDA"
  - "תכנות מקבילי"
series: "חומרת AI & תשתיות"
previousPost: "ai-hardware-infrastructure/2-nvidia-ai"
nextPost: "ai-hardware-infrastructure/4-what-is-an-accelerator"
slug: "3-cuda-nvidia"
---


# CUDA - הכלי שהפך את ה-GPU לנגיש לכולם

GPU היו קיימים הרבה לפני שאנחנו התחלנו להשתמש בהם ל-AI.
אבל עד שנת 2006, אם רצית לכתוב קוד שרץ על GPU - היית צריך לדעת OpenGL או DirectX,
לחשוב כמו מעבד גרפי, ולהתעסק עם שפות ייעודיות ומורכבות.

NVIDIA שינתה את זה כשהשיקה את **CUDA** - ומעצם השקתו, העולם של AI השתנה.

## מה זה CUDA?

**CUDA** (Compute Unified Device Architecture) היא פלטפורמה שמאפשרת לך לכתוב קוד ב-C/C++/Python,
ולהריץ אותו על אלפי ליבות של GPU - במקביל.

במקום להתעסק עם גרפיקה, אתה יכול פשוט לכתוב פונקציה רגילה שמחשבת כפל מטריצות,
ו-CUDA מטפלת בכל השאר: בחלוקת העבודה בין אלפי ליבות, בסנכרון, ובניהול זיכרון.

## איך זה שינה את עולם ה-AI?

לפני CUDA, אימון רשתות נוירונים היה איטי ומסורבל.
אחרי CUDA - הופיע PyTorch, TensorFlow, והמפלצות האלה החלו להשתמש ב-GPU ישירות.

זה היה המפתח:
לא צריך יותר לדעת את CUDA עצמו -
אפשר לכתוב קוד Python רגיל, והוא אוטומטית ירוץ מהר פי 100 על ה-GPU.

## מה CUDA נותן לך?

- **חישוב מקבילי מסיבי** - אלפי ליבות עובדים יחד על אותה משימה.
- **ספריות מובנות** - cuBLAS (אלגברה לינארית), cuDNN (רשתות עמוקות), TensorRT (אופטימיזציה).
- **תמיכה מובנית** - PyTorch, TensorFlow, JAX, MXNet - כולם בנויים על CUDA.
- **פרופיילינג וניפוי שגיאות** - כלים כמו NSight שמאפשרים לראות מה קורה בפנים.

## איך זה עובד בפועל?

נניח שכתבת ב-PyTorch:

```python
x = torch.randn(1000, 1000).cuda()
y = torch.randn(1000, 1000).cuda()
z = x @ y  # matrix multiplication
```

מאחורי הקלעים:

1. PyTorch קורא לספריית cuBLAS של CUDA.
2. הנתונים מועברים לזיכרון ה-GPU.
3. המכפלה מתחשבת בצורה מקבילית על אלפי ליבות.
4. התוצאה חוזרת אליך.

הכול בשבריר שנייה, ובלי שאתה צריך להבין את כל הפרטים הפנימיים.

## יש חלופות ל-CUDA?

- **ROCm** - של AMD, תומך בכרטיסים שלהם.
- **OneAPI** - של Intel, מנסה לייצר סטנדרט פתוח.
- **OpenCL** - פתוח, אבל פחות יעיל ופחות נתמך.

אבל אין ספק: **CUDA עדיין הסטנדרט בפועל**.
כמעט כל ספריית AI נבנתה קודם כול עבור CUDA,
והתמיכה הרשמית חזקה במיוחד.

## סיכום

CUDA היא הגשר בין ה-GPU לבין המפתח.
בלעדיה, רוב מה שאנחנו יודעים היום על AI פשוט לא היה קורה.
ובזכותה, אפילו מפתח שלא יודע כלום על חומרה - יכול לאמן מודל בזמן שיא.

---

**בפוסט הבא נדבר על המונח המרכזי בכל תשתית AI - מה זה בעצם "מאיץ" (Accelerator), ולמה בלעדיו מודלים פשוט לא זזים.**
