---
language: "he"
title: "Concurrency - איך גורמים למערכת לעבוד על כמה דברים במקביל"
categories:
  - "אופטימיזציית הסקה"
tags:
  - "Concurrency"
previousPost: "vllm-efficient-serving"
slug: "concurrency-inference"
---


# Concurrency - איך גורמים למערכת לעבוד על כמה דברים במקביל

נניח שאתם מריצים מודל שמקבל הרבה בקשות משתמשים בו-זמנית.
אם נטפל בכל בקשה אחת-אחת, כל המשתמשים יחכו המון זמן.
Concurrency נועדה לפתור בדיוק את זה - לגרום למערכת לטפל בכמה בקשות במקביל.

## אז מה זה בעצם?

Concurrency פירושו שהמערכת מתזמנת כמה פעולות בו-זמנית,
גם אם הן לא באמת רצות בדיוק באותו רגע (זה כבר Parallelism).

אפשר לחשוב על זה כמו מסעדה:

- יש טבח אחד (GPU או CPU).
- במקום לבשל מנה אחת מהתחלה ועד הסוף ואז להתחיל את הבאה,
  הוא עובד “במחזורים” - חותך ירקות למנה אחת בזמן שמים רותחים לשנייה.

## למה זה חשוב ב-Inference?

כשמודל AI עונה להרבה משתמשים, Concurrency קובע כמה בקשות פעילות הוא מסוגל לטפל בהן בו-זמנית.
אם נגדיר ערך נמוך מדי - המאיץ יתבטל חלק מהזמן.
אם נגדיר גבוה מדי - ניצור עומס, וכל בקשה תתעכב.

המטרה: למצוא את האיזון בין Throughput (כמה בקשות בשנייה) לבין Latency (זמן תגובה ממוצע).

## איך זה עובד בפועל?

שרתים כמו Triton או vLLM משתמשים במנגנונים חכמים:

- **Concurrency Slots** - כמה בקשות אפשר לפתוח במקביל.
- **Dynamic Batching** - איחוד בקשות דומות ל-batch אחד כדי לנצל טוב יותר את המאיץ.
- **Scheduler** - מתאם את הזרימה, כדי שכל המאיץ יעבוד ברציפות, בלי בזבוז.

## בשורה התחתונה

Concurrency הוא אחד הכלים המרכזיים באופטימיזציית inference:
הוא לא רק גורם למערכת לרוץ מהר יותר - הוא גם מנצל טוב יותר את החומרה שכבר יש לך.
