---
language: "he"
title: "vLLM - איך גורמים למודלים לדבר מהר, בלי לבזבז זיכרון"
categories:
  - "vLLM"
tags:
  - "מנוע הסקה"
  - "Serving"
previousPost: "serving-ai"
slug: "vllm-efficient-serving"
---


# vLLM - איך גורמים למודלים לדבר מהר, בלי לבזבז זיכרון

שמעתם על vLLM?
זו אחת הספריות החשובות בעולם ה-inference של מודלים גדולים.
המטרה שלה פשוטה:
להאיץ את תהליך יצירת הטוקנים - כלומר לגרום למודל לענות מהר יותר, תוך שימוש חכם יותר בזיכרון.

## למה צריך את זה בכלל?

כשמודלים גדולים (LLMs) מריצים תשובות, הם יוצרים טוקן אחר טוקן.
הבעיה היא שבדרך כלל הרבה עבודה חוזרת על עצמה -
המערכת שומרת חישובים, אך לא תמיד יודעת לשתף אותם בין בקשות שונות.

vLLM נבנתה בדיוק כדי לפתור את זה:
להשתמש באותו מידע שוב ושוב, בלי חישוב חוזר ובלי לבזבז זיכרון יקר.

## מה מיוחד ב-vLLM?

### PagedAttention
מנגנון ניהול זיכרון חכם שמחלק את ה-KV Cache (זיכרון של תשובות קודמות) לעמודים קטנים.
כך אפשר לשתף חלקים רלוונטיים של הזיכרון בין בקשות שונות,
כמו “שכפול יעיל” של תוצאות ביניים בלי לשמור הכול פעמיים.

### Continuous Batching
מאפשר למערכת לקלוט בקשות חדשות תוך כדי עיבוד בקשות קיימות -
במקום לחכות ש”הבאצ’” יסתיים.
התוצאה: המאיץ עובד כל הזמן, בלי Idle Time כמעט.

### יעילות גבוהה מאוד בזיכרון וב-throughput
הודות לשני המנגנונים האלו, אפשר להפעיל הרבה יותר בקשות על אותו GPU
בלי לאבד מהירות או להעמיס על הזיכרון.

## איפה זה משתלב?

vLLM משתלב היום במערכות inference מודרניות רבות:

- שרתי API של חברות AI.
- מערכות כמו Hugging Face, OpenAI-compatible APIs.
- אינטגרציות עם Kubernetes או Ray Serve להרצה ב-scale.

## בשורה התחתונה

vLLM הופך את ה-Serving של מודלים גדולים ליעיל באמת:
פחות בזבוז, יותר מהירות, ויכולת להריץ מאות משתמשים במקביל -
על אותה חומרה.
