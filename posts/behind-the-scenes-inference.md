---
language: "he"
title: "מה קורה מאחורי הקלעים כשהמודל עונה לך? (Prefill, Decoding ו־KV Cache)"
categories:
  - "למידת מכונה"
  - "טכנולוגיה"
tags:
  - "הסקה"
  - "אופטימיזציה"
previousPost: "measuring-ai-model-speed"
nextPost: "increasing-throughput"
slug: "behind-the-scenes-inference"
---


# מה קורה מאחורי הקלעים כשהמודל עונה לך? (Prefill, Decoding ו־KV Cache)

כשאת שולחת שאלה למודל שפה, זה נראה כאילו הוא פשוט “ממציא” תשובה ברצף —
אבל בפועל, יש שני שלבים עיקריים בתהליך שנקרא Inference:

## שלב 1: Prefill

בשלב הזה המודל מקבל את כל הקלט שלך (ה־prompt) ומחשב את הייצוג הפנימי שלו.
זה כמו שמישהו קורא את כל השאלה שלך לפני שהוא מתחיל לענות.
בשלב הזה מתבצעים חישובים רבים — אבל הוא קורה רק פעם אחת לכל שאלה.

## שלב 2: Decoding

עכשיו מתחילה היצירה בפועל — המודל מייצר כל טוקן חדש, אחד אחרי השני.
כל טוקן תלוי במה שנוצר לפניו, ולכן נדרש לזכור את ההיסטוריה.

## KV Cache

זהו מעין “פנקס זיכרון” שבו נשמרים תוצאות ביניים של השכבות הפנימיות.
במקום לחשב הכול מחדש בכל פעם — המודל פשוט שומר את מה שכבר עבד עליו.

- היתרון: חיסכון משמעותי בזמן ובחישוב.
- האתגר: ככל שהתשובה מתארכת, גם ה־cache גדל ותופס יותר זיכרון.
  אם הגישה אליו איטית — זה עצמו הופך לצוואר בקבוק.

## למה זה חשוב?

ניהול נכון של KV Cache הוא אחד המפתחות להאצת inference במודלים גדולים.
חברות חומרה ותוכנה משקיעות מאמצים עצומים באופטימיזציה שלו –
כי הוא משפיע ישירות על throughput (כמה טוקנים לשנייה ניתן להפיק).